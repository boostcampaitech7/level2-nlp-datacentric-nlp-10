{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from kiwipiepy import Kiwi\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/data/ephemeral/home/yujin/processed_train_4220.csv\")\n",
    "df = df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 클러스터의 데이터 개수 계산\n",
    "label_counts = df['target'].value_counts().sort_index()\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values, palette='husl')\n",
    "\n",
    "# 그래프에 레이블 추가\n",
    "for i, count in enumerate(label_counts.values):\n",
    "    plt.text(i, count, str(count), ha='center', va='bottom')\n",
    "\n",
    "# 그래프 설정\n",
    "plt.title('Number of Data Points in Each Label')\n",
    "plt.xlabel('label')\n",
    "plt.ylabel('Number of Data Points')\n",
    "plt.xticks(label_counts.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kiwi = Kiwi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사 분류\n",
    "def part_classification(text):\n",
    "    \"\"\"\n",
    "    텍스트의 품사를 태깅(분류)하는 함수\n",
    "\n",
    "    Args:\n",
    "        text: 데이터셋의 'text'열의 데이터\n",
    "\n",
    "    Returns:\n",
    "        raw_part_class: 각 텍스트의 토큰에 대한 품사 정보\n",
    "    \"\"\"\n",
    "    raw_part_class = kiwi.tokenize(text, normalize_coda=False)\n",
    "    return raw_part_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주요 품사 필터링 (명사, 동사, 형용사, 어근)\n",
    "MAIN_PART = ['NNG', 'NNB', 'NNP', 'VV', 'VA', 'XR'] # 필터링할 주요 품사\n",
    "VERB_ADJ_PART = ['VV', 'VA'] # 동사, 형용사\n",
    "\n",
    "def part_filtering(text):\n",
    "    \"\"\"\n",
    "    텍스트에서 명사, 동사, 형용사, 어근 품사만 추출하여 필터링하는 함수\n",
    "    \"\"\"\n",
    "    tokens = kiwi.tokenize(text, normalize_coda=False)\n",
    "    class_filter = [\n",
    "        token.form + '다' if token.tag in VERB_ADJ_PART else token.form\n",
    "        for token in tokens\n",
    "        if token.tag in MAIN_PART and len(token.form) > 1\n",
    "    ]\n",
    "    return ' '.join(class_filter)  # 조사를 제거한 단어를 공백으로 연결\n",
    "\n",
    "df['filtered_text'] = df['text'].apply(part_filtering)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 벡터화\n",
    "tfidf_vect = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)  # 유용한 feature 제한\n",
    "tfidf_matrix = tfidf_vect.fit_transform(df['filtered_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means 클러스터링 (카테고리가 7개라고 가정)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 7\n",
    "kmeans = KMeans(n_clusters=num_clusters, max_iter=10000, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 클러스터별로 주요 단어 추출\n",
    "def get_top_keywords(tfidf_matrix, cluster_labels, n_terms=50):\n",
    "    df_keywords = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vect.get_feature_names_out())\n",
    "    df_keywords['cluster'] = cluster_labels\n",
    "    top_keywords = {}\n",
    "\n",
    "    for cluster in range(num_clusters):\n",
    "        top_keywords[cluster] = (\n",
    "            df_keywords[df_keywords['cluster'] == cluster]\n",
    "            .drop('cluster', axis=1)\n",
    "            .mean()\n",
    "            .sort_values(ascending=False)\n",
    "            .head(n_terms)\n",
    "            .index.tolist()\n",
    "        )\n",
    "    return top_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 클러스터별 주요 키워드 확인\n",
    "top_keywords = get_top_keywords(tfidf_matrix, df['cluster'])\n",
    "print(\"클러스터별 주요 키워드:\")\n",
    "for cluster, keywords in top_keywords.items():\n",
    "    print(f\"Cluster {cluster}: {keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 클러스터의 주요 단어 빈도 계산\n",
    "def get_cluster_word_counts(tfidf_matrix, cluster_labels):\n",
    "    \"\"\"\n",
    "    각 클러스터의 단어 빈도를 계산하는 함수\n",
    "\n",
    "    Args:\n",
    "        tfidf_matrix: TF-IDF로 벡터화된 텍스트 행렬\n",
    "        cluster_labels: 각 텍스트의 클러스터 라벨\n",
    "\n",
    "    Returns:\n",
    "        cluster_word_counts: 각 클러스터별 단어 빈도를 저장한 리스트\n",
    "    \"\"\"\n",
    "    # TF-IDF 행렬을 데이터프레임으로 변환하여 단어 이름을 매칭\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vect.get_feature_names_out())\n",
    "    tfidf_df['cluster'] = cluster_labels\n",
    "    \n",
    "    # 각 클러스터별 단어 빈도 계산\n",
    "    cluster_word_counts = {}\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_df = tfidf_df[tfidf_df['cluster'] == cluster].drop('cluster', axis=1)\n",
    "        word_counts = cluster_df.sum(axis=0).sort_values(ascending=False)\n",
    "        cluster_word_counts[cluster] = word_counts[word_counts > 0].index.tolist()\n",
    "        \n",
    "    return cluster_word_counts\n",
    "\n",
    "# 클러스터별 단어 빈도 계산\n",
    "cluster_word_counts = get_cluster_word_counts(tfidf_matrix, df['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클러스터 간 단어 중복 및 고유 단어 확인\n",
    "def get_unique_and_common_words(cluster_word_counts):\n",
    "    \"\"\"\n",
    "    클러스터별 고유 단어와 중복 단어를 구분하는 함수\n",
    "\n",
    "    Args:\n",
    "        cluster_word_counts: 각 클러스터별 단어 리스트\n",
    "\n",
    "    Returns:\n",
    "        unique_words: 클러스터별 고유 단어 딕셔너리\n",
    "        common_words: 모든 클러스터에 중복된 단어 리스트\n",
    "    \"\"\"\n",
    "    # 모든 클러스터의 단어들을 합집합으로 모아 공통 단어와 고유 단어 계산\n",
    "    all_words = set().union(*cluster_word_counts.values())\n",
    "    common_words = set(all_words)\n",
    "    \n",
    "    unique_words = {}\n",
    "    for cluster, words in cluster_word_counts.items():\n",
    "        other_clusters_words = set().union(\n",
    "            *[cluster_word_counts[other] for other in cluster_word_counts if other != cluster]\n",
    "        )\n",
    "        unique_words[cluster] = list(set(words) - other_clusters_words)\n",
    "        common_words &= set(words)\n",
    "    \n",
    "    return unique_words, list(common_words)\n",
    "\n",
    "# 각 클러스터별 고유 단어와 모든 클러스터에 중복되는 단어 확인\n",
    "unique_words, common_words = get_unique_and_common_words(cluster_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 출력\n",
    "print(\"클러스터별 고유 단어:\")\n",
    "for cluster, words in unique_words.items():\n",
    "    print(f\"Cluster {cluster}: {words[:30]}\")  # 상위 10개의 고유 단어만 출력\n",
    "\n",
    "print(\"\\n모든 클러스터에 중복된 단어:\")\n",
    "print(common_words[:30])  # 상위 10개의 중복 단어만 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
