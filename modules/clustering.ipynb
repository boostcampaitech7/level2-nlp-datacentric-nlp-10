{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from kiwipiepy import Kiwi\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"/data/ephemeral/home/yujin/processed_train_4220.csv\")\n",
    "df = pd.read_csv(\"/data/ephemeral/home/level2-nlp-datacentric-nlp-10/train_denoised_none.csv\")\n",
    "\n",
    "df = df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kiwi = Kiwi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트를 형태소 단위로 나누고 필요한 품사만 필터링\n",
    "def preprocess_text(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    filtered_tokens = [token.form for token in tokens if token.tag in ['NNG', 'NNP', 'VV', 'VA', 'XR']]\n",
    "\n",
    "    # # 영어 토큰화\n",
    "    # english_tokens = word_tokenize(text)  # NLTK를 사용하여 영어 단어 토큰화\n",
    "    # # 영어 단어는 필요에 따라 추가적인 필터링을 할 수 있습니다. 예: 영문 단어 길이가 1 이상인 경우\n",
    "    # english_tokens = [word for word in english_tokens if len(word) > 1]\n",
    "    \n",
    "    # # 한국어와 영어 토큰 결합\n",
    "    # combined_tokens = filtered_tokens + english_tokens\n",
    "    # return ' '.join(combined_tokens)\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# 전체 데이터에 대해 전처리 적용\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 stop words 리스트 설정 (예시)\n",
    "korean_stopwords = [\n",
    "    '을', '를', '는', '에', '의', '다', '있다', '하다', \n",
    "    '그', '저', '그들', '저희', '이런', '어떤', '모든', '자신', '지금', \n",
    "    '또', '이렇게', '그런', '그리고', '하지만', '그러나', '또한', '때문에'\n",
    "]\n",
    "\n",
    "# CountVectorizer로 텍스트를 벡터화\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=korean_stopwords)\n",
    "dtm = vectorizer.fit_transform(df['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA 모델 학습\n",
    "num_topics = 7  # 주제의 개수를 설정 (예: 카테고리가 7개인 경우)\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda_model.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문서에 대해 주제 확률 추출\n",
    "df['topic'] = lda_model.transform(dtm).argmax(axis=1)\n",
    "\n",
    "# 결과 확인\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주제별 상위 단어 확인\n",
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}: \", \" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "\n",
    "display_topics(lda_model, vectorizer.get_feature_names_out(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyLDAvis 시각화\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# 직접 LDA 결과를 시각화\n",
    "panel = pyLDAvis.prepare(\n",
    "    topic_term_dists=lda_model.components_ / lda_model.components_.sum(axis=1)[:, None],\n",
    "    doc_topic_dists=lda_model.transform(dtm),\n",
    "    doc_lengths=[len(doc.split()) for doc in df['processed_text']],\n",
    "    vocab=vectorizer.get_feature_names_out(),\n",
    "    term_frequency=dtm.sum(axis=0).getA1()\n",
    ")\n",
    "pyLDAvis.display(panel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
