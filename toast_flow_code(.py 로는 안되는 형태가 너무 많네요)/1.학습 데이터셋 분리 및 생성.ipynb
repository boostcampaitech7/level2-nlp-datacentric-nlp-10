{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 노이즈 복원 모델 학습용을 위한 노이즈 없는 데이터만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 945개의 정상 텍스트를 추출하였습니다.\n",
      "총 텍스트 수: 2800\n",
      "한글 비율로 제외된 텍스트 수: 1609\n",
      "특수 문자 비율로 제외된 텍스트 수: 184\n",
      "이상 패턴으로 제외된 텍스트 수: 62\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터셋 로드 => 초기 데이터셋\n",
    "data = pd.read_csv('/data/ephemeral/data/train.csv')\n",
    "\n",
    "# 필터링된 정상 텍스트를 저장할 리스트\n",
    "normal_texts = []\n",
    "id = []\n",
    "\n",
    "# 통계 정보 => 텍스트 노이즈 분류 기준\n",
    "total_texts = 0\n",
    "excluded_due_to_korean_ratio = 0\n",
    "excluded_due_to_special_ratio = 0\n",
    "excluded_due_to_abnormal_pattern = 0\n",
    "\n",
    "\n",
    "for text in data['text']:\n",
    "    # 텍스트를 문자열로 변환 (NaN 값 처리)\n",
    "    if not isinstance(text, str):\n",
    "        continue\n",
    "    \n",
    "    total_texts += 1\n",
    "\n",
    "    # 텍스트에서 공백 제거\n",
    "    text_no_space = text.replace(' ', '')\n",
    "    \n",
    "    # 텍스트 길이 계산 ,공백 및 특수문자 제외\n",
    "    total_chars = re.findall(r'[A-Za-z0-9가-힣]', text_no_space)\n",
    "    total_length = len(total_chars)\n",
    "    if total_length == 0:\n",
    "        continue\n",
    "    \n",
    "    # 한글, 영어, 숫자, 특수 문자 개수 계산\n",
    "    korean_chars = re.findall(r'[가-힣]', text_no_space)\n",
    "    special_chars = re.findall(r'[^A-Za-z0-9가-힣\\s]', text_no_space)\n",
    "    \n",
    "    korean_ratio = len(korean_chars) / total_length\n",
    "    special_ratio = len(special_chars) / (len(text_no_space))\n",
    "    \n",
    "    # 이상 패턴 검사 => 데이터 보고 추가 조건을 정규표현식으로 추가\n",
    "    abnormal_patterns = [\n",
    "        r'[A-Za-z]{3,}[0-9]{3,}',       # 영어 3자 이상 + 숫자 3자 이상 (너무 짧은 패턴 제외)\n",
    "        r'[0-9]{3,}[A-Za-z]{3,}',       # 숫자 3자 이상 + 영어 3자 이상\n",
    "        r'[A-Za-z]{2,}[^A-Za-z0-9가-힣\\s]{2,}',  # 영어 2자 이상 + 특수문자 2자 이상\n",
    "        r'[^A-Za-z0-9가-힣\\s]{2,}[A-Za-z]{2,}',  # 특수문자 2자 이상 + 영어 2자 이상\n",
    "        r'[A-Za-z]{2,}-[0-9]+-[A-Za-z]{2,}',  # 영어 2자 이상 - 숫자 - 영어 2자 이상\n",
    "        r'[^A-Za-z0-9가-힣\\s]{3,}',    # 연속된 특수 문자 3자 이상\n",
    "        r'^(?=[^{}\\(\\*\\[\\]]*[}\\(\\*\\[][^{}\\(\\*\\[\\]]*$)',  # 특정 기호가 하나만 포함된 문자열 필터링\n",
    "        r'[A-Za-z]{1,2}[^A-Za-z0-9가-힣\\s]+[가-힣]',  # 영어 1~2자 + 특수문자 + 한글\n",
    "        r'[가-힣]+[^A-Za-z0-9가-힣\\s]+[A-Za-z]{1,2}',  # 한글 + 특수문자 + 영어 1~2자\n",
    "        r'[가-힣]+[^A-Za-z0-9가-힣\\s]+[가-힣]+[A-Za-z]',  # 한글 + 특수문자 + 한글 + 영어  \n",
    "        r'[^A-Za-z0-9가-힣\\s]+[0-9]+[^A-Za-z0-9가-힣\\s]+' # 특수문자 + 숫자 + 특수문자 \n",
    "        r''\n",
    "    ]\n",
    "    \n",
    "    has_abnormal_pattern = False\n",
    "    for pattern in abnormal_patterns:\n",
    "        if re.search(pattern, text_no_space):\n",
    "            has_abnormal_pattern = True\n",
    "            break\n",
    "    \n",
    "    # 필터링 조건 설정\n",
    "    if korean_ratio < 0.8:\n",
    "        excluded_due_to_korean_ratio += 1\n",
    "        continue\n",
    "    if special_ratio > 0.1:\n",
    "        excluded_due_to_special_ratio += 1\n",
    "        continue\n",
    "    if has_abnormal_pattern:\n",
    "        excluded_due_to_abnormal_pattern += 1\n",
    "        continue\n",
    "\n",
    "    # 통과한 텍스트 저장\n",
    "    normal_texts.append(text)\n",
    "    \n",
    "    \n",
    "\n",
    "# 결과를 데이터프레임으로 변환하여 저장\n",
    "normal_texts_df = pd.DataFrame({'text': normal_texts})\n",
    "normal_texts_df.to_csv('normal_texts.csv', index=False)\n",
    "\n",
    "print(f\"총 {len(normal_texts)}개의 정상 텍스트를 추출하였습니다.\")\n",
    "print(f\"총 텍스트 수: {total_texts}\")\n",
    "print(f\"한글 비율로 제외된 텍스트 수: {excluded_due_to_korean_ratio}\")\n",
    "print(f\"특수 문자 비율로 제외된 텍스트 수: {excluded_due_to_special_ratio}\")\n",
    "print(f\"이상 패턴으로 제외된 텍스트 수: {excluded_due_to_abnormal_pattern}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라벨링 용 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 1296개의 정상 텍스트를 추출하였습니다.\n",
      "총 1504개의 노이즈 텍스트를 추출하였습니다.\n",
      "총 텍스트 수: 2800\n",
      "한글 비율로 제외된 텍스트 수: 748\n",
      "특수 문자 비율로 제외된 텍스트 수: 1370\n",
      "이상 패턴으로 제외된 텍스트 수: 297\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터셋 로드 \n",
    "data = pd.read_csv('/data/ephemeral/data/train.csv')\n",
    "\n",
    "# 필터링된 정상 텍스트와 ID를 저장할 리스트\n",
    "normal_texts = []\n",
    "normal_ids = []\n",
    "\n",
    "# 노이즈가 있는 텍스트와 ID를 저장할 리스트\n",
    "noisy_texts = []\n",
    "noisy_ids = []\n",
    "\n",
    "# 통계 정보\n",
    "total_texts = 0\n",
    "excluded_due_to_korean_ratio = 0\n",
    "excluded_due_to_special_ratio = 0\n",
    "excluded_due_to_abnormal_pattern = 0\n",
    "\n",
    "# 각 텍스트에 대한 분석 및 필터링\n",
    "for index, row in data.iterrows():\n",
    "    text = row['text']\n",
    "    text_id = row['ID']  \n",
    "\n",
    "    # 텍스트를 문자열로 변환\n",
    "    if not isinstance(text, str):\n",
    "        continue\n",
    "    \n",
    "    total_texts += 1\n",
    "\n",
    "    # 텍스트에서 공백 제거\n",
    "    text_no_space = text.replace(' ', '')\n",
    "    \n",
    "    # 텍스트 길이 계산 \n",
    "    total_chars = re.findall(r'[A-Za-z0-9가-힣]', text_no_space)\n",
    "    total_length = len(total_chars)\n",
    "    if total_length == 0:\n",
    "        continue\n",
    "    \n",
    "    # 한글, 영어, 숫자, 특수 문자 개수 계산\n",
    "    korean_chars = re.findall(r'[가-힣]', text_no_space)\n",
    "    special_chars = re.findall(r'[^A-Za-z0-9가-힣\\s]', text_no_space)\n",
    "    \n",
    "    korean_ratio = len(korean_chars) / total_length\n",
    "    special_ratio = len(special_chars) / len(text_no_space)\n",
    "    \n",
    "    # 이상 패턴 검사 \n",
    "    # 이때 조건을 좀더 원활하게 하여 노이즈 데이터에 노이즈 없는 데이터가 들어가지 않도록 수정 => 학습 시 좀더 원활하게 하기 위해\n",
    "    abnormal_patterns = [\n",
    "        r'[A-Za-z]{3,}[0-9]{3,}',       # 영어 3자 이상 + 숫자 3자 이상 (너무 짧은 패턴 제외)\n",
    "        r'[0-9]{3,}[A-Za-z]{3,}',       # 숫자 3자 이상 + 영어 3자 이상\n",
    "        r'[A-Za-z]{2,}[^A-Za-z0-9가-힣\\s]{2,}',  # 영어 2자 이상 + 특수문자 2자 이상\n",
    "        r'[^A-Za-z0-9가-힣\\s]{2,}[A-Za-z]{2,}',  # 특수문자 2자 이상 + 영어 2자 이상\n",
    "        r'[A-Za-z]{2,}-[0-9]+-[A-Za-z]{2,}',  # 영어 2자 이상 - 숫자 - 영어 2자 이상\n",
    "        r'[^A-Za-z0-9가-힣\\s]{3,}',    # 연속된 특수 문자 3자 이상\n",
    "        # r'^(?=[^{}\\(\\*\\[\\]]*[}\\(\\*\\[][^{}\\(\\*\\[\\]]*$)',  # 특정 기호가 하나만 포함된 문자열 필터링\n",
    "        # r'[A-Za-z]{1,2}[^A-Za-z0-9가-힣\\s]+[가-힣]',  # 영어 1~2자 + 특수문자 + 한글\n",
    "        # r'[가-힣]+[^A-Za-z0-9가-힣\\s]+[A-Za-z]{1,2}',  # 한글 + 특수문자 + 영어 1~2자\n",
    "        # r'[가-힣]+[^A-Za-z0-9가-힣\\s]+[가-힣]+[A-Za-z]',  # 한글 + 특수문자 + 한글 + 영어  \n",
    "        # r'[^A-Za-z0-9가-힣\\s]+[0-9]+[^A-Za-z0-9가-힣\\s]+', # 특수문자 + 숫자 + 특수문자 \n",
    "    ]\n",
    "    \n",
    "    has_abnormal_pattern = False\n",
    "    for pattern in abnormal_patterns:\n",
    "        if re.search(pattern, text_no_space):\n",
    "            has_abnormal_pattern = True\n",
    "            break\n",
    "    \n",
    "    # 필터링 조건 설정\n",
    "    if korean_ratio < 0.6 or special_ratio > 0.1 or has_abnormal_pattern:\n",
    "        noisy_texts.append(text)\n",
    "        noisy_ids.append(text_id)\n",
    "        if korean_ratio < 0.6:\n",
    "            excluded_due_to_korean_ratio += 1\n",
    "        if special_ratio > 0.1:\n",
    "            excluded_due_to_special_ratio += 1\n",
    "        if has_abnormal_pattern:\n",
    "            excluded_due_to_abnormal_pattern += 1\n",
    "        continue\n",
    "\n",
    "    # 통과한 텍스트와 ID 저장\n",
    "    normal_texts.append(text)\n",
    "    normal_ids.append(text_id)\n",
    "\n",
    "# 정상 텍스트와 노이즈 텍스트를 각각 데이터프레임으로 변환하여 저장\n",
    "normal_texts_df = pd.DataFrame({'ID': normal_ids, 'text': normal_texts})\n",
    "noisy_texts_df = pd.DataFrame({'ID': noisy_ids, 'text': noisy_texts})\n",
    "\n",
    "normal_texts_df.to_csv('/data/ephemeral/data/normal_texts_ID.csv', index=False)\n",
    "noisy_texts_df.to_csv('/data/ephemeral/data/noisy_texts_ID.csv', index=False)\n",
    "\n",
    "print(f\"총 {len(normal_texts)}개의 정상 텍스트를 추출하였습니다.\")\n",
    "print(f\"총 {len(noisy_texts)}개의 노이즈 텍스트를 추출하였습니다.\")\n",
    "print(f\"총 텍스트 수: {total_texts}\")\n",
    "print(f\"한글 비율로 제외된 텍스트 수: {excluded_due_to_korean_ratio}\")\n",
    "print(f\"특수 문자 비율로 제외된 텍스트 수: {excluded_due_to_special_ratio}\")\n",
    "print(f\"이상 패턴으로 제외된 텍스트 수: {excluded_due_to_abnormal_pattern}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM 으로 학습하기 위해 노이즈 데이터 생성 구조와 비슷한 체계로 노이즈 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "\n",
    "# 데이터셋 로드\n",
    "data = pd.read_csv('/data/ephemeral/jung/normal_texts.csv')  # 깨끗한 데이터셋 파일명\n",
    "\n",
    "# 노이즈 데이터를 저장할 리스트\n",
    "noisy_texts = []\n",
    "\n",
    "# 특수 문자 포함한 아스키 문자 집합 생성\n",
    "ascii_chars = string.printable \n",
    "\n",
    "def add_noise_to_text(text, min_ratio=0.1, max_ratio=0.5):\n",
    "    text_chars = list(text)\n",
    "    total_length = len(text_chars)\n",
    "    if total_length == 0:\n",
    "        return text  \n",
    "    \n",
    "    # 대체할 문자 수 결정 (10~50% 범위 내에서 랜덤하게 결정) => 80%는 노이즈가 너무 심해 LLM 이 제대로 학습하지 못하는 경향이 있으므로 제외\n",
    "    replace_ratio = random.uniform(min_ratio, max_ratio)\n",
    "    num_chars_to_replace = max(1, int(total_length * replace_ratio))  # 최소 1개는 교체\n",
    "\n",
    "    # 대체할 위치를 랜덤으로 선택\n",
    "    indices = list(range(total_length))\n",
    "    random.shuffle(indices)\n",
    "    indices_to_replace = indices[:num_chars_to_replace]\n",
    "    \n",
    "    # 문자 대체\n",
    "    for idx in indices_to_replace:\n",
    "        # 임의의 아스키 문자 선택\n",
    "        random_ascii_char = random.choice(ascii_chars)\n",
    "        text_chars[idx] = random_ascii_char\n",
    "    \n",
    "    noisy_text = ''.join(text_chars)\n",
    "    return noisy_text\n",
    "\n",
    "# 노이즈 데이터 생성\n",
    "for text in data['text']:\n",
    "    noisy_text = add_noise_to_text(text)\n",
    "    noisy_texts.append(noisy_text)\n",
    "\n",
    "# 노이즈-원본 데이터쌍 생성\n",
    "data['noisy_text'] = noisy_texts\n",
    "\n",
    "# 결과 저장\n",
    "data[['noisy_text', 'text']].to_csv('noisy_clean_pairs.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
