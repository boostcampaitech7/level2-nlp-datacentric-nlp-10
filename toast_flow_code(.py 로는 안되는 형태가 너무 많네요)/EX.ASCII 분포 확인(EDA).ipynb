{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 456\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, '../data')\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, '../output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 바꾸지 말것\n",
    "model_name = 'klue/bert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이즈 조정은 편하대로 \n",
    "data = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
    "dataset_train, dataset_valid = train_test_split(data, test_size=0.3, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724     1B금융o차기 O장 최종Q보I UQ\"…&실=)>7임 성공g합\n",
      "1939               &0L0년이면x-공지능L_건강·P률 상담\n",
      "2720                 미래 I업 연~소 =험<9 초등학생들\n",
      "283                    부산국제코미디페스티벌에 참여합니다\n",
      "805         고용안정지원금 오프라인 신청 첫날…한숨 돌리게 됐어요\n",
      "                      ...                \n",
      "2543       박기원 감독 눈치 보지 말고…비예나 눈치 본 건 아닌데\n",
      "2090      R성 베F남] 갤~6@JsJl개…중x가폰으p 동남아 공H\n",
      "2649                  아이팩토리 상장폐지 이의신청서 제출\n",
      "613                LG전자 미국서 G6 사면 구글 홈 준다\n",
      "1947           미래에셋대우 엔씨소프트 매출 1위 굳건…목표가↑\n",
      "Name: text, Length: 1960, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       ID                               text  target  \\\n",
      "724   ynat-v1_train_00724  1B금융o차기 O장 최종Q보I UQ\"…&실=)>7임 성공g합       5   \n",
      "1939  ynat-v1_train_01939             &0L0년이면x-공지능L_건강·P률 상담       4   \n",
      "2720  ynat-v1_train_02720               미래 I업 연~소 =험<9 초등학생들       3   \n",
      "283   ynat-v1_train_00283                 부산국제코미디페스티벌에 참여합니다       3   \n",
      "805   ynat-v1_train_00805      고용안정지원금 오프라인 신청 첫날…한숨 돌리게 됐어요       4   \n",
      "...                   ...                                ...     ...   \n",
      "2543  ynat-v1_train_02543     박기원 감독 눈치 보지 말고…비예나 눈치 본 건 아닌데       3   \n",
      "2090  ynat-v1_train_02090    R성 베F남] 갤~6@JsJl개…중x가폰으p 동남아 공H       4   \n",
      "2649  ynat-v1_train_02649                아이팩토리 상장폐지 이의신청서 제출       1   \n",
      "613   ynat-v1_train_00613             LG전자 미국서 G6 사면 구글 홈 준다       4   \n",
      "1947  ynat-v1_train_01947         미래에셋대우 엔씨소프트 매출 1위 굳건…목표가↑       5   \n",
      "\n",
      "                        text_cleaned  \n",
      "724            금융 차기 장 최종 보 실 임 성공 합  \n",
      "1939                 년이면 공지능 건강 률 상담  \n",
      "2720                미래 업 연 소 험 초등학생들  \n",
      "283               부산국제코미디페스티벌에 참여합니다  \n",
      "805    고용안정지원금 오프라인 신청 첫날 한숨 돌리게 됐어요  \n",
      "...                              ...  \n",
      "2543  박기원 감독 눈치 보지 말고 비예나 눈치 본 건 아닌데  \n",
      "2090          성 베 남 갤 개 중 가폰으 동남아 공   \n",
      "2649             아이팩토리 상장폐지 이의신청서 제출  \n",
      "613                전자 미국서 사면 구글 홈 준다  \n",
      "1947       미래에셋대우 엔씨소프트 매출 위 굳건 목표가   \n",
      "\n",
      "[1960 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 한국어 문자가 아닌 모든 코드 제거 => 유니코드 , 아스키 코드 범위에서 제거\n",
    "non_korean_pattern = re.compile(r'[^\\uac00-\\ud7a3]+')\n",
    "\n",
    "dataset_train['text_cleaned'] = dataset_train['text'].apply(lambda x: non_korean_pattern.sub('[Mask]', str(x)))\n",
    "\n",
    "print(dataset_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [105, 32, 58, 49, 32, 122, 32, 75, 84, 40, 32,...\n",
      "1    [75, 46, 68, 76, 119, 111, 32, 76, 51, 78, 37,...\n",
      "2           [109, 32, 41, 32, 32, 44, 63, 114, 49, 49]\n",
      "3                 [56, 32, 32, 50, 55, 32, 32, 32, 32]\n",
      "4    [112, 73, 73, 32, 82, 50, 102, 114, 32, 93, 32...\n",
      "Name: ascii_info, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_csv(\"/data/ephemeral/data/train.csv\")\n",
    "\n",
    "# 각 텍스트에서 ASCII 코드로 변환된 문자들의 아스키 범위 탐색\n",
    "def get_ascii_info(text):\n",
    "    ascii_chars = [ord(char) for char in text if ord(char) < 128]\n",
    "    return ascii_chars\n",
    "\n",
    "data['ascii_info'] = data['text'].apply(get_ascii_info)\n",
    "\n",
    "# 아스키 코드 범위 출력\n",
    "print(data['ascii_info'].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               text  \\\n",
      "0  정i :파1 미사z KT( 이용기간 2e 단] Q분종U2보   \n",
      "1       K찰.국DLwo 로L3한N% 회장 2 T0&}송=   \n",
      "2            m 김정) 자주통일 새,?r열1나가야1보   \n",
      "3     갤노트8 주말 27만대 개통…시장은 불법 보조금 얼룩   \n",
      "4      pI美대선I앞두고 R2fr단 발] $비해 감시 강화   \n",
      "\n",
      "                               replacement_positions  \n",
      "0  [1, 2, 3, 5, 6, 9, 10, 11, 12, 13, 14, 19, 20,...  \n",
      "1  [0, 2, 4, 5, 6, 7, 8, 10, 11, 13, 14, 15, 18, ...  \n",
      "2               [0, 1, 4, 5, 10, 12, 13, 14, 16, 20]  \n",
      "3                    [3, 4, 7, 8, 9, 12, 19, 22, 26]  \n",
      "4  [0, 1, 5, 9, 10, 11, 12, 13, 15, 17, 18, 19, 2...  \n"
     ]
    }
   ],
   "source": [
    "# ASCII로 대체된 문자의 위치 찾기\n",
    "def find_replacement_positions(text):\n",
    "    return [i for i, char in enumerate(text) if ord(char) < 128]\n",
    "\n",
    "data['replacement_positions'] = data['text'].apply(find_replacement_positions)\n",
    "print(data[['text', 'replacement_positions']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               text  \\\n",
      "0  정i :파1 미사z KT( 이용기간 2e 단] Q분종U2보   \n",
      "1       K찰.국DLwo 로L3한N% 회장 2 T0&}송=   \n",
      "2            m 김정) 자주통일 새,?r열1나가야1보   \n",
      "3     갤노트8 주말 27만대 개통…시장은 불법 보조금 얼룩   \n",
      "4      pI美대선I앞두고 R2fr단 발] $비해 감시 강화   \n",
      "\n",
      "                                             bigrams  \\\n",
      "0  [ 2,  :,  k,  q,  단,  미,  이, ( , 1 , 2e, 2보, :...   \n",
      "1  [ 2,  t,  로,  회, % , &}, .국, 0&, 2 , 3한, dl, k...   \n",
      "2  [ 김,  새,  자, ) , ,?, 1나, 1보, ?r, m , r열, 가야, 김...   \n",
      "3  [ 2,  개,  보,  불,  얼,  주, 27, 7만, 8 , …시, 개통, 갤...   \n",
      "4  [ $,  r,  감,  강,  발, $비, 2f, ] , fr, i美, i앞, p...   \n",
      "\n",
      "                                            trigrams  \n",
      "0  [ 2e,  :파,  kt,  q분,  단],  미사,  이용, ( 이, 1 미, ...  \n",
      "1  [ 2 ,  t0,  로l,  회장, % 회, &}송, .국d, 0&}, 2 t, ...  \n",
      "2  [ 김정,  새,,  자주, ) 자, ,?r, 1나가, ?r열, m 김, r열1, ...  \n",
      "3  [ 27,  개통,  보조,  불법,  얼룩,  주말, 27만, 7만대, 8 주, ...  \n",
      "4  [ $비,  r2,  감시,  강화,  발], $비해, 2fr, ] $, fr단, ...  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# N-그램 추출 함수\n",
    "def extract_n_grams(text, n=2):\n",
    "    vectorizer = CountVectorizer(analyzer='char', ngram_range=(n, n))\n",
    "    n_grams = vectorizer.fit_transform([text])\n",
    "    return vectorizer.get_feature_names_out()\n",
    "\n",
    "# 각 텍스트에서 2-그램 추출\n",
    "data['bigrams'] = data['text'].apply(lambda x: extract_n_grams(x, n=2))\n",
    "data['trigrams'] = data['text'].apply(lambda x: extract_n_grams(x, n=3))\n",
    "\n",
    "# N-그램 예시 출력\n",
    "print(data[['text', 'bigrams', 'trigrams']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               text  \\\n",
      "0  정i :파1 미사z KT( 이용기간 2e 단] Q분종U2보   \n",
      "1       K찰.국DLwo 로L3한N% 회장 2 T0&}송=   \n",
      "2            m 김정) 자주통일 새,?r열1나가야1보   \n",
      "3     갤노트8 주말 27만대 개통…시장은 불법 보조금 얼룩   \n",
      "4      pI美대선I앞두고 R2fr단 발] $비해 감시 강화   \n",
      "\n",
      "                                  non_korean_bigrams  \\\n",
      "0  [ 2,  :,  k,  q, ( , 1 , 2e, ] , e , i , kt, t...   \n",
      "1  [ 2,  t, % , &}, 0&, 2 , dl, l3, lw, n%, o , t...   \n",
      "2                                   [) , ,?, ?r, m ]   \n",
      "3                                       [ 2, 27, 8 ]   \n",
      "4                   [ $,  r, 2f, ] , fr, i美, pi, r2]   \n",
      "\n",
      "                             non_korean_trigrams  \n",
      "0       [ 2e,  kt, 2e , ] q, i :, kt(, t( , z k]  \n",
      "1  [ 2 ,  t0, 0&}, 2 t, dlw, lwo, n% , t0&, wo ]  \n",
      "2                                          [,?r]  \n",
      "3                                          [ 27]  \n",
      "4                      [ r2, 2fr, ] $, pi美, r2f]  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 한글과 아닌 N-그램 필터링\n",
    "def filter_korean_ngrams(n_grams):\n",
    "    non_korean_ngrams = [gram for gram in n_grams if not re.search(r'[가-힣]', gram)]\n",
    "    return non_korean_ngrams\n",
    "\n",
    "# 한글이 없는 N-그램 추출\n",
    "data['non_korean_bigrams'] = data['bigrams'].apply(filter_korean_ngrams)\n",
    "data['non_korean_trigrams'] = data['trigrams'].apply(filter_korean_ngrams)\n",
    "\n",
    "# 필터링 결과 출력\n",
    "print(data[['text', 'non_korean_bigrams', 'non_korean_trigrams']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top non-Korean bigrams: [(' 1', 219), (' 2', 164), (' 3', 118), ('t ', 106), (' 4', 94), ('s ', 94), (' 5', 90), (' a', 76), ('c ', 76), (' k', 73)]\n",
      "Top non-Korean trigrams: [('...', 44), ('kt ', 39), (' 10', 30), (' 20', 22), ('skt', 19), ('mlb', 17), ('5g ', 17), ('lg ', 16), (' ai', 16), (' lg', 15)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 비한글 N-그램 빈도 계산\n",
    "bigrams = [gram for grams in data['non_korean_bigrams'] for gram in grams]\n",
    "trigrams = [gram for grams in data['non_korean_trigrams'] for gram in grams]\n",
    "\n",
    "# 빈도 계산\n",
    "bigram_counts = Counter(bigrams)\n",
    "trigram_counts = Counter(trigrams)\n",
    "\n",
    "# 상위 빈도 N-그램 출력\n",
    "print(\"Top non-Korean bigrams:\", bigram_counts.most_common(10))\n",
    "print(\"Top non-Korean trigrams:\", trigram_counts.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
